{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Report 1\n",
    "----\n",
    "\n",
    "**Team Members**\n",
    "\n",
    "Yaqian Cheng, Department of Statistical Science\n",
    "\n",
    "Mengrun Li, Department of Statistical Science\n",
    "\n",
    "**Github repository**\n",
    "\n",
    "<https://github.com/cici7941/Sta_663_Statistical_Computation_Final_Project>\n",
    "\n",
    "**Choice of paper** \n",
    "\n",
    "*Scalable K-Means++*\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "*K-means* is one of the most popular clustering methods. A good initialization of *k-means* is essential for obtaining the global optimal solution and efficiency. However, there are two main obstacles with traditional *k-means* method. One is theoretical inefficiency and the other one is that its final solution is locally optimal. A better algorithm, *k-means++* addresses the second problem with an improved initialization procedure of the cluster centers. But this *k-means++* initialization is not parallelizable, because the selection for the *i*th center depends on the previous *i-1* centers [1]. Therefore, *k-means||*, a parallelizable version of *k-means++*, has been raised, which can both improve the final solution and run faster. In this report, we implemented the algorithm in the paper \"Scalable K-Means++\" in Python, compared the clustering cost and runtime between *k-means*, *k-means++* and *k-means||*, performed tests for main functions, profiled the performance of the algorithm and identified bottlenecks, and performed optimization using Cython. We then apply *k-means||* to a massive dataset to evaluate its performance.\n",
    "\n",
    "**Outline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "2. Algorithm  \n",
    "    2.1 K-Means  \n",
    "    2.2 K-Means++  \n",
    "    2.3 K-Means||  \n",
    "3. Code Testing\n",
    "4. Profiling and Optimization\n",
    "5. Application and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def euc_dist(x, y):\n",
    "    return la.norm(x-y)\n",
    "\n",
    "def centroid(X):\n",
    "    return X.mean(0)\n",
    "    \n",
    "def d(x, Y):\n",
    "    minDist = float(\"inf\")\n",
    "    for yi in Y:\n",
    "        dist = euc_dist(x, yi)\n",
    "        if(dist < minDist):\n",
    "            minDist = dist\n",
    "    return minDist\n",
    "\n",
    "def cost(Y, C):\n",
    "    cost = 0\n",
    "    for yi in Y:\n",
    "        cost += d(yi, C)**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-Means++\n",
    "def kmeansPlus(data, k):\n",
    "    idx = np.random.choice(data.shape[0], 1)\n",
    "    C = data[idx, :]\n",
    "    while(C.shape[0] < k):\n",
    "        prob = [d(xi,C)**2 for xi in data]/cost(data,C)\n",
    "        new = data[np.random.choice(data.shape[0], size=1, p=prob),:]\n",
    "        if(new.tolist() not in C.tolist()):\n",
    "            C = np.r_[C, new]\n",
    "    return C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
